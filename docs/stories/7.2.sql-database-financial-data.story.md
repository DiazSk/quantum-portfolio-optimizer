# Story 7.2: SQL Database Skills & Financial Data

**Story ID**: STORY-7.2  
**Epic**: EPIC-007 (Entry-Level Data Analyst Portfolio)  
**Created**: August 20, 2025  
**Product Owner**: Sarah  
**Scrum Master**: Bob  
**Sprint**: Sprint 2 (September 2-6, 2025)  
**Story Points**: 8  
**Priority**: Critical - Database Foundation  
**Status**: Ready for Development

---

## 📋 **Story Description**

### **User Story**
As a **data analyst intern candidate**, I want to **demonstrate advanced SQL querying and database design skills with financial data** so that I can **confidently handle database-related interview questions and data extraction tasks**.

### **Context**
SQL is the most universally required skill for data analyst positions. This story establishes proficiency in complex queries, database design, and working with financial datasets that showcase real-world business applications.

### **Business Value**
- Meet SQL requirements for 95%+ of data analyst job postings
- Demonstrate ability to work with complex relational data
- Show understanding of financial data structures and relationships
- Create portfolio pieces that discuss data architecture decisions

---

## ✅ **Acceptance Criteria**

### **AC-7.2.1: Database Design & Setup**
- **Given** financial market data requirements
- **When** designing a portfolio tracking database
- **Then** create normalized database schema with proper relationships
- **And** implement tables for stocks, portfolios, transactions, and prices
- **And** establish foreign key constraints and data integrity rules
- **And** document database design decisions and trade-offs

### **AC-7.2.2: Complex Query Development**
- **Given** populated financial database
- **When** performing analytical queries
- **Then** write multi-table joins for portfolio performance analysis
- **And** implement window functions for moving averages and rankings
- **And** create subqueries for relative performance calculations
- **And** use aggregate functions for risk and return metrics

### **AC-7.2.3: Financial Data Analysis**
- **Given** stock price and transaction data
- **When** calculating investment metrics
- **Then** compute portfolio returns and asset allocation
- **And** calculate risk metrics (volatility, max drawdown, VaR)
- **And** identify top/bottom performers with ranking queries
- **And** analyze transaction patterns and trading frequency

### **AC-7.2.4: Data Pipeline & ETL**
- **Given** external data sources
- **When** building automated data collection
- **Then** create stored procedures for data loading and validation
- **And** implement error handling for data quality issues
- **And** schedule automated data updates and maintenance
- **And** create data lineage documentation

### **AC-7.2.5: Performance Optimization**
- **Given** large financial datasets
- **When** optimizing query performance
- **Then** create appropriate indexes for common query patterns
- **And** optimize complex queries for sub-second response times
- **And** implement query execution plan analysis
- **And** document performance tuning decisions

---

## 🔧 **Technical Requirements**

### **Database Schema Design**
```sql
-- Portfolio tracking database schema
CREATE DATABASE portfolio_tracker;

-- Companies and stock information
CREATE TABLE companies (
    company_id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) UNIQUE NOT NULL,
    company_name VARCHAR(255) NOT NULL,
    sector VARCHAR(100),
    industry VARCHAR(100),
    market_cap BIGINT,
    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Daily stock prices
CREATE TABLE stock_prices (
    price_id SERIAL PRIMARY KEY,
    company_id INTEGER REFERENCES companies(company_id),
    price_date DATE NOT NULL,
    open_price DECIMAL(10,2),
    high_price DECIMAL(10,2),
    low_price DECIMAL(10,2),
    close_price DECIMAL(10,2),
    adj_close_price DECIMAL(10,2),
    volume BIGINT,
    UNIQUE(company_id, price_date)
);

-- Portfolio definitions
CREATE TABLE portfolios (
    portfolio_id SERIAL PRIMARY KEY,
    portfolio_name VARCHAR(255) NOT NULL,
    description TEXT,
    creation_date DATE DEFAULT CURRENT_DATE,
    initial_value DECIMAL(15,2),
    benchmark_symbol VARCHAR(10)
);

-- Portfolio holdings
CREATE TABLE holdings (
    holding_id SERIAL PRIMARY KEY,
    portfolio_id INTEGER REFERENCES portfolios(portfolio_id),
    company_id INTEGER REFERENCES companies(company_id),
    shares DECIMAL(15,4),
    average_cost DECIMAL(10,2),
    purchase_date DATE,
    target_weight DECIMAL(5,4)
);

-- Transaction history
CREATE TABLE transactions (
    transaction_id SERIAL PRIMARY KEY,
    portfolio_id INTEGER REFERENCES portfolios(portfolio_id),
    company_id INTEGER REFERENCES companies(company_id),
    transaction_type VARCHAR(10) CHECK (transaction_type IN ('BUY', 'SELL')),
    shares DECIMAL(15,4),
    price DECIMAL(10,2),
    transaction_date DATE,
    commission DECIMAL(8,2) DEFAULT 0
);
```

### **Advanced SQL Analysis Queries**

#### **Portfolio Performance Analysis**
```sql
-- Calculate portfolio daily returns with benchmark comparison
WITH portfolio_values AS (
    SELECT 
        p.portfolio_id,
        p.portfolio_name,
        sp.price_date,
        SUM(h.shares * sp.close_price) as portfolio_value
    FROM portfolios p
    JOIN holdings h ON p.portfolio_id = h.portfolio_id
    JOIN companies c ON h.company_id = c.company_id
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date >= '2024-01-01'
    GROUP BY p.portfolio_id, p.portfolio_name, sp.price_date
),
portfolio_returns AS (
    SELECT 
        portfolio_id,
        portfolio_name,
        price_date,
        portfolio_value,
        LAG(portfolio_value) OVER (PARTITION BY portfolio_id ORDER BY price_date) as prev_value,
        (portfolio_value / LAG(portfolio_value) OVER (PARTITION BY portfolio_id ORDER BY price_date) - 1) as daily_return
    FROM portfolio_values
)
SELECT 
    portfolio_name,
    COUNT(*) as trading_days,
    AVG(daily_return) * 252 as annualized_return,
    STDDEV(daily_return) * SQRT(252) as annualized_volatility,
    AVG(daily_return) / STDDEV(daily_return) * SQRT(252) as sharpe_ratio,
    MIN(daily_return) as worst_day,
    MAX(daily_return) as best_day
FROM portfolio_returns
WHERE daily_return IS NOT NULL
GROUP BY portfolio_id, portfolio_name
ORDER BY annualized_return DESC;
```

#### **Risk Analysis Queries**
```sql
-- Calculate Value at Risk (VaR) and maximum drawdown
WITH daily_returns AS (
    SELECT 
        h.portfolio_id,
        sp.price_date,
        SUM(h.shares * sp.close_price / LAG(sp.close_price) OVER (PARTITION BY sp.company_id ORDER BY sp.price_date) - 1) as portfolio_return
    FROM holdings h
    JOIN companies c ON h.company_id = c.company_id
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date >= '2023-01-01'
    GROUP BY h.portfolio_id, sp.price_date
),
cumulative_returns AS (
    SELECT 
        portfolio_id,
        price_date,
        portfolio_return,
        SUM(portfolio_return) OVER (PARTITION BY portfolio_id ORDER BY price_date) as cumulative_return,
        MAX(SUM(portfolio_return) OVER (PARTITION BY portfolio_id ORDER BY price_date)) 
            OVER (PARTITION BY portfolio_id ORDER BY price_date ROWS UNBOUNDED PRECEDING) as running_max
    FROM daily_returns
    WHERE portfolio_return IS NOT NULL
)
SELECT 
    portfolio_id,
    PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY portfolio_return) as var_5_percent,
    PERCENTILE_CONT(0.01) WITHIN GROUP (ORDER BY portfolio_return) as var_1_percent,
    MIN(cumulative_return - running_max) as max_drawdown,
    AVG(portfolio_return) as mean_return,
    STDDEV(portfolio_return) as volatility
FROM cumulative_returns
GROUP BY portfolio_id;
```

#### **Sector Analysis and Correlation**
```sql
-- Analyze sector performance and correlations
WITH sector_returns AS (
    SELECT 
        c.sector,
        sp.price_date,
        AVG(sp.close_price / LAG(sp.close_price) OVER (PARTITION BY sp.company_id ORDER BY sp.price_date) - 1) as sector_return
    FROM companies c
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date >= '2024-01-01'
    GROUP BY c.sector, sp.price_date
),
sector_performance AS (
    SELECT 
        sector,
        COUNT(*) as trading_days,
        AVG(sector_return) * 252 as annualized_return,
        STDDEV(sector_return) * SQRT(252) as annualized_volatility,
        MIN(sector_return) as worst_day,
        MAX(sector_return) as best_day
    FROM sector_returns
    WHERE sector_return IS NOT NULL
    GROUP BY sector
)
SELECT 
    sector,
    ROUND(annualized_return::NUMERIC, 4) as annual_return,
    ROUND(annualized_volatility::NUMERIC, 4) as annual_volatility,
    ROUND((annualized_return / annualized_volatility)::NUMERIC, 4) as risk_adjusted_return,
    ROUND(worst_day::NUMERIC, 4) as worst_day_return,
    ROUND(best_day::NUMERIC, 4) as best_day_return
FROM sector_performance
ORDER BY annualized_return DESC;
```

### **Data Pipeline and ETL Scripts**
```sql
-- Stored procedure for daily data loading
CREATE OR REPLACE FUNCTION load_daily_prices(load_date DATE)
RETURNS TABLE(loaded_count INTEGER, error_count INTEGER) AS $$
DECLARE
    loaded_count INTEGER := 0;
    error_count INTEGER := 0;
BEGIN
    -- Data validation and loading logic
    INSERT INTO stock_prices (company_id, price_date, open_price, high_price, low_price, close_price, volume)
    SELECT 
        c.company_id,
        load_date,
        -- Data loading logic here
    FROM companies c
    WHERE c.symbol IN (SELECT symbol FROM daily_data_staging)
    ON CONFLICT (company_id, price_date) DO UPDATE SET
        open_price = EXCLUDED.open_price,
        high_price = EXCLUDED.high_price,
        low_price = EXCLUDED.low_price,
        close_price = EXCLUDED.close_price,
        volume = EXCLUDED.volume;
    
    GET DIAGNOSTICS loaded_count = ROW_COUNT;
    
    RETURN QUERY SELECT loaded_count, error_count;
END;
$$ LANGUAGE plpgsql;

-- Data quality validation function
CREATE OR REPLACE FUNCTION validate_price_data()
RETURNS TABLE(company_symbol VARCHAR, issue_description TEXT) AS $$
BEGIN
    RETURN QUERY
    -- Check for missing data
    SELECT c.symbol, 'Missing recent price data' as issue
    FROM companies c
    LEFT JOIN stock_prices sp ON c.company_id = sp.company_id 
        AND sp.price_date > CURRENT_DATE - INTERVAL '7 days'
    WHERE sp.company_id IS NULL
    
    UNION ALL
    
    -- Check for data anomalies
    SELECT c.symbol, 'Unusual price movement (>50% change)' as issue
    FROM companies c
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date = CURRENT_DATE - 1
    AND ABS(sp.close_price / 
        (SELECT close_price FROM stock_prices sp2 
         WHERE sp2.company_id = c.company_id 
         AND sp2.price_date = CURRENT_DATE - 2) - 1) > 0.5;
END;
$$ LANGUAGE plpgsql;
```

---

## 📊 **Deliverables & Portfolio Items**

### **Database Design Documentation**
1. **ERD (Entity Relationship Diagram)**: Visual database schema design
2. **Data Dictionary**: Comprehensive table and column documentation
3. **Normalization Analysis**: Explanation of database design decisions
4. **Performance Considerations**: Index strategy and query optimization

### **SQL Query Portfolio**
1. **Basic Analytics**: Revenue, growth, and performance queries
2. **Advanced Analytics**: Risk metrics, correlations, and rankings
3. **Reporting Queries**: Executive dashboard and KPI calculations
4. **Data Pipeline**: ETL processes and data validation scripts

### **Analysis Reports**
1. **Portfolio Performance Report**: Comprehensive investment analysis
2. **Risk Assessment Report**: VaR, drawdown, and volatility analysis
3. **Sector Analysis Report**: Industry performance comparison
4. **Data Quality Report**: Validation results and recommendations

---

## 🧪 **Testing & Validation**

### **Data Integrity Tests**
- **Referential Integrity**: Verify foreign key relationships
- **Data Quality**: Check for nulls, duplicates, and outliers
- **Business Rules**: Validate financial calculations and constraints
- **Performance Tests**: Query execution time under various data volumes

### **Query Accuracy Validation**
- **Benchmark Comparison**: Validate calculations against known results
- **Cross-Verification**: Compare SQL results with Python calculations
- **Edge Case Testing**: Handle missing data and market holidays
- **Historical Validation**: Verify results against published financial data

---

## 📚 **Learning Objectives**

### **SQL Mastery**
- **Advanced Joins**: Multi-table relationships and complex join conditions
- **Window Functions**: Ranking, moving averages, and cumulative calculations
- **Subqueries & CTEs**: Complex analytical queries with proper structure
- **Performance Optimization**: Index design and query tuning

### **Database Design**
- **Normalization**: Proper table structure and relationship design
- **Data Modeling**: Financial domain modeling and business rules
- **Constraints**: Data integrity and validation at the database level
- **ETL Processes**: Data pipeline design and automation

---

## 🎯 **Interview Preparation**

### **Technical Discussion Points**
- **Schema Design**: Explain database normalization and design decisions
- **Query Optimization**: Discuss index usage and performance tuning
- **Data Quality**: Explain validation approaches and error handling
- **Scalability**: Address performance considerations for large datasets

### **Business Application**
- **Financial Metrics**: Explain calculation methodology and business meaning
- **Risk Analysis**: Discuss risk metrics and their interpretation
- **Reporting**: Demonstrate ability to create executive-level summaries
- **Data Pipeline**: Explain ETL design and automation approaches

---

## ⚠️ **Risks and Mitigation**

### **Technical Risks**
1. **Database Performance**: Complex queries may run slowly
   - **Mitigation**: Implement proper indexing and query optimization
   
2. **Data Quality Issues**: Financial data may have gaps or errors
   - **Mitigation**: Robust validation and error handling procedures

### **Learning Risks**
1. **SQL Complexity**: Advanced features may be challenging to master
   - **Mitigation**: Start with basic queries and progressively add complexity
   
2. **Database Setup**: Installing and configuring database software
   - **Mitigation**: Use cloud database services or Docker containers

---

## 🎯 **Definition of Done**

### **Technical Completion**
- [ ] Complete database schema with 5+ normalized tables
- [ ] 20+ SQL queries demonstrating various skill levels
- [ ] Working ETL pipeline with data validation
- [ ] Performance-optimized queries with proper indexing
- [ ] Comprehensive documentation of all components

### **Portfolio Readiness**
- [ ] GitHub repository with complete SQL scripts
- [ ] Database setup instructions and sample data
- [ ] Query explanation and business context documentation
- [ ] Performance metrics and optimization examples
- [ ] Ready for technical interview whiteboard exercises

### **Learning Validation**
- [ ] Can write complex analytical queries independently
- [ ] Can explain database design decisions clearly
- [ ] Comfortable with query optimization and performance tuning
- [ ] Ready to discuss financial data modeling approaches
- [ ] Confident in SQL technical interview scenarios

---

**Sprint Planning Notes for Bob:**
- Set up PostgreSQL or MySQL database environment early in sprint
- Focus on commonly tested SQL interview questions and patterns
- Ensure all queries have clear business context and explanations
- Plan for code review session focusing on query optimization
- Consider using real financial data APIs for realistic scenarios
