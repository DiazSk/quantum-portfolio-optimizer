# Story 8.1: Technical Interview Preparation

**Story ID**: STORY-8.1  
**Epic**: EPIC-008 (Interview Preparation & Application Strategy)  
**Created**: August 20, 2025  
**Product Owner**: Sarah  
**Scrum Master**: Bob  
**Sprint**: Sprint 5 (September 23-27, 2025)  
**Story Points**: 8  
**Priority**: Critical - Interview Success  
**Status**: Ready for Development

---

## ðŸ“‹ **Story Description**

### **User Story**
As a **data analyst intern candidate**, I want to **master technical interview skills and coding challenges** so that I can **confidently handle technical interviews and demonstrate my analytical problem-solving abilities to potential employers**.

### **Context**
Technical interviews for data analyst positions typically include live coding challenges, SQL problems, statistical questions, and portfolio project walkthroughs. This story prepares comprehensive responses to common technical interview scenarios and builds confidence for real-time problem solving.

### **Business Value**
- Increase interview success rate through systematic preparation
- Demonstrate technical competency under pressure
- Build confidence for live coding and problem-solving scenarios
- Create reusable preparation materials for multiple interview cycles

---

## âœ… **Acceptance Criteria**

### **AC-8.1.1: SQL Interview Mastery**
- **Given** SQL coding challenges commonly used in data analyst interviews
- **When** solving problems under time pressure
- **Then** complete 50+ SQL problems covering joins, aggregations, and window functions
- **And** solve complex multi-table queries within 15-minute time limits
- **And** explain query logic and optimization strategies clearly
- **And** handle follow-up questions about alternative solutions

### **AC-8.1.2: Python Data Analysis Challenges**
- **Given** take-home and live coding scenarios
- **When** performing data analysis tasks
- **Then** complete pandas data manipulation challenges in <30 minutes
- **And** solve statistical calculation problems with proper methodology
- **And** create visualizations that answer specific business questions
- **And** write clean, commented code suitable for code review

### **AC-8.1.3: Statistical & Probability Questions**
- **Given** verbal statistical reasoning questions
- **When** explaining statistical concepts and applications
- **Then** answer probability questions with clear explanations
- **And** explain statistical significance and hypothesis testing
- **And** discuss correlation vs. causation with business examples
- **And** interpret A/B testing results and make recommendations

### **AC-8.1.4: Portfolio Project Presentations**
- **Given** completed portfolio projects from Epic 7
- **When** presenting work in interview settings
- **Then** deliver compelling 5-minute project overviews
- **And** handle technical deep-dive questions about methodology
- **And** explain business impact and insights generated
- **And** discuss challenges faced and solutions implemented

### **AC-8.1.5: Case Study Problem Solving**
- **Given** business scenarios requiring analytical solutions
- **When** working through case study problems
- **Then** break down complex business problems systematically
- **And** design appropriate analytical approaches for different scenarios
- **And** communicate assumptions and limitations clearly
- **And** provide actionable recommendations based on analysis

---

## ðŸ”§ **Technical Interview Preparation Framework**

### **SQL Interview Challenge Bank**

#### **Category 1: Basic to Intermediate Queries**
```sql
-- Problem 1: Top Performing Stocks
-- Find the top 5 stocks by total return over the last year
SELECT 
    c.symbol,
    c.company_name,
    ((MAX(sp.close_price) - MIN(sp.close_price)) / MIN(sp.close_price)) * 100 as total_return_pct
FROM companies c
JOIN stock_prices sp ON c.company_id = sp.company_id
WHERE sp.price_date >= CURRENT_DATE - INTERVAL '1 year'
GROUP BY c.symbol, c.company_name
ORDER BY total_return_pct DESC
LIMIT 5;

-- Problem 2: Portfolio Rebalancing Analysis
-- Identify holdings that are more than 5% away from target allocation
WITH current_allocations AS (
    SELECT 
        p.portfolio_name,
        c.symbol,
        h.shares * sp.close_price as current_value,
        SUM(h.shares * sp.close_price) OVER (PARTITION BY p.portfolio_id) as total_portfolio_value,
        h.target_weight
    FROM portfolios p
    JOIN holdings h ON p.portfolio_id = h.portfolio_id
    JOIN companies c ON h.company_id = c.company_id
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date = (SELECT MAX(price_date) FROM stock_prices)
)
SELECT 
    portfolio_name,
    symbol,
    ROUND((current_value / total_portfolio_value)::NUMERIC, 4) as current_weight,
    target_weight,
    ROUND((current_value / total_portfolio_value - target_weight)::NUMERIC, 4) as weight_difference
FROM current_allocations
WHERE ABS(current_value / total_portfolio_value - target_weight) > 0.05
ORDER BY ABS(current_value / total_portfolio_value - target_weight) DESC;
```

#### **Category 2: Advanced Window Functions**
```sql
-- Problem 3: Rolling Performance Metrics
-- Calculate 30-day rolling Sharpe ratio for each stock
WITH daily_returns AS (
    SELECT 
        c.symbol,
        sp.price_date,
        sp.close_price / LAG(sp.close_price) OVER (PARTITION BY c.symbol ORDER BY sp.price_date) - 1 as daily_return
    FROM companies c
    JOIN stock_prices sp ON c.company_id = sp.company_id
),
rolling_metrics AS (
    SELECT 
        symbol,
        price_date,
        daily_return,
        AVG(daily_return) OVER (PARTITION BY symbol ORDER BY price_date ROWS 29 PRECEDING) as rolling_mean,
        STDDEV(daily_return) OVER (PARTITION BY symbol ORDER BY price_date ROWS 29 PRECEDING) as rolling_std
    FROM daily_returns
    WHERE daily_return IS NOT NULL
)
SELECT 
    symbol,
    price_date,
    CASE 
        WHEN rolling_std > 0 THEN rolling_mean / rolling_std * SQRT(252)
        ELSE NULL
    END as rolling_sharpe_ratio
FROM rolling_metrics
WHERE price_date >= '2024-01-01'
ORDER BY symbol, price_date;

-- Problem 4: Sector Momentum Analysis
-- Rank sectors by momentum (3-month vs 1-month performance)
WITH sector_returns AS (
    SELECT 
        c.sector,
        sp.price_date,
        AVG(sp.close_price / LAG(sp.close_price, 21) OVER (PARTITION BY c.company_id ORDER BY sp.price_date) - 1) as monthly_return,
        AVG(sp.close_price / LAG(sp.close_price, 63) OVER (PARTITION BY c.company_id ORDER BY sp.price_date) - 1) as quarterly_return
    FROM companies c
    JOIN stock_prices sp ON c.company_id = sp.company_id
    WHERE sp.price_date = (SELECT MAX(price_date) FROM stock_prices)
    GROUP BY c.sector, sp.price_date
)
SELECT 
    sector,
    ROUND(monthly_return::NUMERIC, 4) as one_month_return,
    ROUND(quarterly_return::NUMERIC, 4) as three_month_return,
    ROUND((quarterly_return - monthly_return)::NUMERIC, 4) as momentum_score,
    RANK() OVER (ORDER BY quarterly_return - monthly_return DESC) as momentum_rank
FROM sector_returns
ORDER BY momentum_rank;
```

### **Python Coding Challenges**

#### **Data Manipulation Challenges**
```python
# Challenge 1: Portfolio Rebalancing Calculator
def calculate_rebalancing_trades(current_holdings, target_weights, total_portfolio_value):
    """
    Calculate trades needed to rebalance portfolio to target weights
    
    Args:
        current_holdings: dict {symbol: current_shares}
        target_weights: dict {symbol: target_weight}
        total_portfolio_value: float
        
    Returns:
        dict {symbol: shares_to_trade} (positive = buy, negative = sell)
    """
    import pandas as pd
    
    # Get current prices (mock data for interview)
    current_prices = {
        'AAPL': 150.00, 'GOOGL': 2500.00, 'MSFT': 300.00,
        'AMZN': 3200.00, 'TSLA': 800.00
    }
    
    trades = {}
    
    for symbol in target_weights:
        # Calculate current value and weight
        current_value = current_holdings.get(symbol, 0) * current_prices[symbol]
        current_weight = current_value / total_portfolio_value
        
        # Calculate target value and required shares
        target_value = target_weights[symbol] * total_portfolio_value
        target_shares = target_value / current_prices[symbol]
        
        # Calculate trade required
        current_shares = current_holdings.get(symbol, 0)
        shares_to_trade = target_shares - current_shares
        
        if abs(shares_to_trade) > 0.1:  # Only trade if significant difference
            trades[symbol] = round(shares_to_trade, 2)
    
    return trades

# Challenge 2: Risk Metrics Calculator
def calculate_portfolio_risk_metrics(returns_series, confidence_levels=[0.95, 0.99]):
    """
    Calculate comprehensive risk metrics for a returns series
    
    Args:
        returns_series: pandas Series of daily returns
        confidence_levels: list of confidence levels for VaR calculation
        
    Returns:
        dict with risk metrics
    """
    import numpy as np
    import pandas as pd
    from scipy import stats
    
    # Basic statistics
    mean_return = returns_series.mean()
    volatility = returns_series.std()
    skewness = stats.skew(returns_series)
    kurtosis = stats.kurtosis(returns_series)
    
    # Value at Risk (VaR) calculations
    var_metrics = {}
    for conf_level in confidence_levels:
        alpha = 1 - conf_level
        # Parametric VaR (assuming normal distribution)
        var_parametric = stats.norm.ppf(alpha) * volatility
        # Historical VaR
        var_historical = np.percentile(returns_series, alpha * 100)
        # Expected Shortfall (Conditional VaR)
        es = returns_series[returns_series <= var_historical].mean()
        
        var_metrics[f'VaR_{int(conf_level*100)}'] = {
            'parametric': var_parametric,
            'historical': var_historical,
            'expected_shortfall': es
        }
    
    # Maximum Drawdown
    cumulative_returns = (1 + returns_series).cumprod()
    rolling_max = cumulative_returns.expanding().max()
    drawdown = (cumulative_returns - rolling_max) / rolling_max
    max_drawdown = drawdown.min()
    
    # Sharpe Ratio (assuming risk-free rate = 0 for simplicity)
    sharpe_ratio = mean_return / volatility * np.sqrt(252) if volatility > 0 else 0
    
    return {
        'mean_daily_return': mean_return,
        'annualized_return': mean_return * 252,
        'daily_volatility': volatility,
        'annualized_volatility': volatility * np.sqrt(252),
        'sharpe_ratio': sharpe_ratio,
        'skewness': skewness,
        'kurtosis': kurtosis,
        'max_drawdown': max_drawdown,
        'var_metrics': var_metrics
    }
```

#### **Statistical Analysis Challenges**
```python
# Challenge 3: A/B Testing Analysis
def analyze_ab_test(control_group, treatment_group, alpha=0.05):
    """
    Analyze A/B test results and provide recommendations
    
    Args:
        control_group: list or array of control group values
        treatment_group: list or array of treatment group values
        alpha: significance level
        
    Returns:
        dict with test results and recommendations
    """
    from scipy import stats
    import numpy as np
    
    # Basic statistics
    control_mean = np.mean(control_group)
    treatment_mean = np.mean(treatment_group)
    control_std = np.std(control_group, ddof=1)
    treatment_std = np.std(treatment_group, ddof=1)
    
    # Effect size
    effect_size = treatment_mean - control_mean
    relative_effect = (effect_size / control_mean) * 100 if control_mean != 0 else 0
    
    # Two-sample t-test
    t_stat, p_value = stats.ttest_ind(treatment_group, control_group)
    
    # Confidence interval for difference
    n1, n2 = len(control_group), len(treatment_group)
    pooled_std = np.sqrt(((n1-1)*control_std**2 + (n2-1)*treatment_std**2) / (n1+n2-2))
    se_diff = pooled_std * np.sqrt(1/n1 + 1/n2)
    df = n1 + n2 - 2
    t_critical = stats.t.ppf(1 - alpha/2, df)
    ci_lower = effect_size - t_critical * se_diff
    ci_upper = effect_size + t_critical * se_diff
    
    # Power analysis (post-hoc)
    cohen_d = effect_size / pooled_std
    
    # Results interpretation
    is_significant = p_value < alpha
    practical_significance = abs(relative_effect) > 5  # 5% threshold for practical significance
    
    recommendation = "implement_treatment" if (is_significant and practical_significance and effect_size > 0) else "no_change"
    
    return {
        'test_statistics': {
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'effect_size': effect_size,
            'relative_effect_pct': relative_effect,
            't_statistic': t_stat,
            'p_value': p_value,
            'confidence_interval': (ci_lower, ci_upper),
            'cohen_d': cohen_d
        },
        'conclusions': {
            'statistically_significant': is_significant,
            'practically_significant': practical_significance,
            'recommendation': recommendation,
            'interpretation': f"Treatment group showed {relative_effect:.2f}% {'improvement' if effect_size > 0 else 'decline'} compared to control"
        }
    }

# Challenge 4: Time Series Trend Analysis
def analyze_time_series_trend(data, date_col, value_col):
    """
    Analyze time series for trends and seasonality
    
    Args:
        data: pandas DataFrame with time series data
        date_col: name of date column
        value_col: name of value column
        
    Returns:
        dict with trend analysis results
    """
    import pandas as pd
    import numpy as np
    from scipy import stats
    
    # Ensure date column is datetime
    data[date_col] = pd.to_datetime(data[date_col])
    data = data.sort_values(date_col)
    
    # Create time index for regression
    data['time_index'] = range(len(data))
    
    # Linear trend analysis
    slope, intercept, r_value, p_value, std_err = stats.linregress(data['time_index'], data[value_col])
    
    # Calculate percentage change over period
    first_value = data[value_col].iloc[0]
    last_value = data[value_col].iloc[-1]
    total_change_pct = ((last_value - first_value) / first_value) * 100 if first_value != 0 else 0
    
    # Moving averages for trend confirmation
    data['ma_7'] = data[value_col].rolling(7).mean()
    data['ma_30'] = data[value_col].rolling(30).mean()
    
    # Trend direction based on slope
    if slope > 0 and p_value < 0.05:
        trend_direction = "increasing"
    elif slope < 0 and p_value < 0.05:
        trend_direction = "decreasing"
    else:
        trend_direction = "no_significant_trend"
    
    # Volatility analysis
    daily_changes = data[value_col].pct_change().dropna()
    volatility = daily_changes.std()
    
    return {
        'trend_analysis': {
            'slope': slope,
            'r_squared': r_value**2,
            'p_value': p_value,
            'trend_direction': trend_direction,
            'total_change_pct': total_change_pct
        },
        'volatility_metrics': {
            'daily_volatility': volatility,
            'annualized_volatility': volatility * np.sqrt(252),
        },
        'moving_averages': {
            'current_vs_ma7': ((last_value - data['ma_7'].iloc[-1]) / data['ma_7'].iloc[-1] * 100) if not pd.isna(data['ma_7'].iloc[-1]) else None,
            'current_vs_ma30': ((last_value - data['ma_30'].iloc[-1]) / data['ma_30'].iloc[-1] * 100) if not pd.isna(data['ma_30'].iloc[-1]) else None,
        }
    }
```

---

## ðŸ“Š **Portfolio Project Presentation Framework**

### **5-Minute Project Walkthrough Template**
```yaml
presentation_structure:
  minute_1:
    - Problem statement and business context
    - Data sources and scope
    - Key objectives
  
  minute_2:
    - Methodology overview
    - Tools and techniques used
    - Analytical approach
  
  minute_3:
    - Key findings and insights
    - Statistical results and significance
    - Visualization highlights
  
  minute_4:
    - Business implications
    - Recommendations and next steps
    - Risk considerations
  
  minute_5:
    - Questions and technical deep-dive
    - Challenges faced and solutions
    - Alternative approaches considered
```

### **Technical Deep-Dive Preparation**
```python
# Prepare for detailed questions about methodology
project_technical_details = {
    'python_analysis': {
        'libraries_used': ['pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn'],
        'data_preprocessing': 'Handled missing values, outliers, and data type conversions',
        'analysis_methods': 'Descriptive statistics, correlation analysis, hypothesis testing',
        'visualization_choices': 'Selected appropriate chart types for data patterns',
        'code_organization': 'Modular functions, clear documentation, version control'
    },
    'sql_analysis': {
        'database_design': 'Normalized schema with proper relationships',
        'query_optimization': 'Appropriate indexes, efficient join strategies',
        'complex_queries': 'Window functions, CTEs, subqueries for advanced analysis',
        'data_validation': 'Constraints, checks, and quality assurance procedures'
    },
    'statistical_methods': {
        'assumption_testing': 'Verified normality, independence, homoscedasticity',
        'hypothesis_testing': 'Appropriate test selection and interpretation',
        'confidence_intervals': 'Calculated and interpreted for key metrics',
        'effect_size': 'Considered practical vs statistical significance'
    }
}
```

---

## ðŸ§ª **Mock Interview Practice Sessions**

### **SQL Live Coding Practice**
- **Format**: 45-minute sessions with progressive difficulty
- **Tools**: DB Fiddle, SQLiteOnline, or local database setup
- **Scenarios**: Portfolio analysis, risk calculations, performance rankings
- **Evaluation**: Code correctness, efficiency, explanation quality

### **Python Data Analysis Challenges**
- **Format**: 30-minute take-home style problems
- **Environment**: Jupyter notebooks with sample datasets
- **Focus**: Data cleaning, analysis, visualization, interpretation
- **Review**: Code quality, methodology, business insights

### **Case Study Problem Solving**
- **Format**: 60-minute business case discussions
- **Scenarios**: Investment decisions, risk assessment, market analysis
- **Approach**: Structured problem breakdown, analytical framework, recommendations
- **Evaluation**: Logical thinking, business acumen, communication skills

---

## ðŸŽ¯ **Definition of Done**

### **Technical Mastery**
- [ ] Complete 50+ SQL problems with 90%+ accuracy
- [ ] Solve Python challenges within time constraints
- [ ] Master statistical concepts with practical applications
- [ ] Deliver confident portfolio project presentations
- [ ] Handle technical deep-dive questions about methodology

### **Interview Readiness**
- [ ] Practice mock interviews with realistic scenarios
- [ ] Prepare clear explanations for all technical choices
- [ ] Develop compelling narratives around project impact
- [ ] Build confidence for live coding under pressure
- [ ] Create backup examples for different interview formats

### **Problem-Solving Demonstration**
- [ ] Show systematic approach to breaking down complex problems
- [ ] Demonstrate ability to validate assumptions and results
- [ ] Communicate technical concepts in business-friendly language
- [ ] Handle uncertainty and incomplete information gracefully
- [ ] Provide actionable recommendations based on analysis

---

**Sprint Planning Notes for Bob:**
- Schedule regular mock interview sessions with different interviewers
- Focus on commonly asked questions and problem types
- Practice explaining technical concepts to non-technical audiences
- Time all practice sessions to build comfort with pressure
- Create contingency explanations for different audience technical levels
- Record practice sessions for self-evaluation and improvement
