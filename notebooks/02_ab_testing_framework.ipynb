{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2e9a60",
   "metadata": {},
   "source": [
    "# üß™ A/B Testing Framework for Portfolio Strategies\n",
    "\n",
    "## üéØ **Objective**\n",
    "Implement rigorous A/B testing methodology to compare portfolio optimization strategies.\n",
    "This demonstrates **FAANG-level experimentation skills** essential for data analyst positions.\n",
    "\n",
    "### üìä **Key Competencies Showcased**\n",
    "- **Experimental Design**: Control/treatment setup with proper randomization\n",
    "- **Statistical Power**: Sample size calculations for reliable conclusions\n",
    "- **Effect Size Estimation**: Practical significance vs. statistical significance\n",
    "- **Multiple Testing**: Family-wise error rate control\n",
    "- **Business Metrics**: ROI, risk-adjusted returns, drawdown analysis\n",
    "\n",
    "### üíº **Business Context**\n",
    "Before deploying new portfolio strategies with real money, we need statistical proof they work.\n",
    "A/B testing provides the framework to make data-driven investment decisions with quantified confidence.\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Data Analytics Team  \n",
    "**Date**: August 2025  \n",
    "**Status**: Production Ready  \n",
    "**Business Impact**: Risk reduction + alpha generation validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7c3d2",
   "metadata": {},
   "source": [
    "## üìö **1. Environment Setup & Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a748da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core analytics and experimentation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical testing and power analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import (\n",
    "    ttest_ind, mannwhitneyu, wilcoxon,\n",
    "    chi2_contingency, fisher_exact,\n",
    "    ks_2samp, anderson_ksamp\n",
    ")\n",
    "from statsmodels.stats.power import ttest_power, ttest_ind_solve_power\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Portfolio and financial analysis\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.portfolio.portfolio_optimizer import PortfolioOptimizer\n",
    "from src.risk.risk_managment import RiskManager\n",
    "\n",
    "# Professional logging and utilities\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Set up professional styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ A/B Testing Framework Initialized\")\n",
    "print(f\"üìÖ Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"üéØ Ready for portfolio strategy experimentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec806a7c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è **2. A/B Testing Framework Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abadc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioABTester:\n",
    "    \"\"\"\n",
    "    Professional A/B testing framework for portfolio strategies\n",
    "    Implements statistical rigor required for financial decision-making\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_level=0.95, minimum_effect_size=0.02):\n",
    "        self.confidence_level = confidence_level\n",
    "        self.alpha = 1 - confidence_level\n",
    "        self.minimum_effect_size = minimum_effect_size  # 2% minimum meaningful difference\n",
    "        self.results = {}\n",
    "        \n",
    "    def calculate_sample_size(self, effect_size=None, power=0.8, ratio=1):\n",
    "        \"\"\"\n",
    "        Calculate required sample size for statistical power\n",
    "        \n",
    "        Args:\n",
    "            effect_size: Expected difference between groups (Cohen's d)\n",
    "            power: Statistical power (1 - Type II error rate)\n",
    "            ratio: Ratio of treatment to control group size\n",
    "        \"\"\"\n",
    "        if effect_size is None:\n",
    "            effect_size = self.minimum_effect_size\n",
    "            \n",
    "        # Calculate sample size using power analysis\n",
    "        n_control = ttest_ind_solve_power(\n",
    "            effect_size=effect_size,\n",
    "            power=power,\n",
    "            alpha=self.alpha,\n",
    "            ratio=ratio\n",
    "        )\n",
    "        \n",
    "        n_treatment = n_control * ratio\n",
    "        \n",
    "        return {\n",
    "            'n_control': int(np.ceil(n_control)),\n",
    "            'n_treatment': int(np.ceil(n_treatment)),\n",
    "            'total_sample_size': int(np.ceil(n_control + n_treatment)),\n",
    "            'effect_size': effect_size,\n",
    "            'power': power,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "    \n",
    "    def run_experiment(self, control_returns, treatment_returns, \n",
    "                      experiment_name=\"Portfolio A/B Test\", metrics=['returns', 'sharpe', 'drawdown']):\n",
    "        \"\"\"\n",
    "        Run comprehensive A/B test comparing two portfolio strategies\n",
    "        \n",
    "        Args:\n",
    "            control_returns: Daily returns for control strategy (baseline)\n",
    "            treatment_returns: Daily returns for treatment strategy (new)\n",
    "            experiment_name: Human-readable experiment identifier\n",
    "            metrics: List of metrics to test ['returns', 'sharpe', 'volatility', 'drawdown']\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'experiment_name': experiment_name,\n",
    "            'start_date': datetime.now().isoformat(),\n",
    "            'sample_sizes': {\n",
    "                'control': len(control_returns),\n",
    "                'treatment': len(treatment_returns)\n",
    "            },\n",
    "            'metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Test each metric\n",
    "        for metric in metrics:\n",
    "            if metric == 'returns':\n",
    "                control_metric = control_returns.mean() * 252  # Annualized\n",
    "                treatment_metric = treatment_returns.mean() * 252\n",
    "                \n",
    "            elif metric == 'sharpe':\n",
    "                control_metric = (control_returns.mean() * 252) / (control_returns.std() * np.sqrt(252))\n",
    "                treatment_metric = (treatment_returns.mean() * 252) / (treatment_returns.std() * np.sqrt(252))\n",
    "                \n",
    "            elif metric == 'volatility':\n",
    "                control_metric = control_returns.std() * np.sqrt(252)\n",
    "                treatment_metric = treatment_returns.std() * np.sqrt(252)\n",
    "                \n",
    "            elif metric == 'drawdown':\n",
    "                control_metric = self._calculate_max_drawdown(control_returns)\n",
    "                treatment_metric = self._calculate_max_drawdown(treatment_returns)\n",
    "            \n",
    "            # Statistical tests\n",
    "            metric_results = self._test_metric_difference(\n",
    "                control_returns, treatment_returns, \n",
    "                control_metric, treatment_metric, metric\n",
    "            )\n",
    "            \n",
    "            results['metrics'][metric] = metric_results\n",
    "        \n",
    "        # Multiple testing correction\n",
    "        p_values = [results['metrics'][m]['p_value'] for m in metrics]\n",
    "        rejected, corrected_p, _, _ = multipletests(p_values, method='bonferroni')\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            results['metrics'][metric]['p_value_corrected'] = corrected_p[i]\n",
    "            results['metrics'][metric]['significant_after_correction'] = rejected[i]\n",
    "        \n",
    "        # Overall experiment conclusion\n",
    "        results['overall_conclusion'] = self._generate_conclusion(results)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[experiment_name] = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _test_metric_difference(self, control_returns, treatment_returns, \n",
    "                               control_metric, treatment_metric, metric_name):\n",
    "        \"\"\"\n",
    "        Perform statistical test for metric difference\n",
    "        \"\"\"\n",
    "        # Two-sample t-test\n",
    "        t_stat, t_p_value = ttest_ind(treatment_returns, control_returns)\n",
    "        \n",
    "        # Non-parametric alternative (Mann-Whitney U)\n",
    "        u_stat, u_p_value = mannwhitneyu(treatment_returns, control_returns, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(control_returns) - 1) * control_returns.var() + \n",
    "                             (len(treatment_returns) - 1) * treatment_returns.var()) / \n",
    "                            (len(control_returns) + len(treatment_returns) - 2))\n",
    "        \n",
    "        cohens_d = (treatment_returns.mean() - control_returns.mean()) / pooled_std\n",
    "        \n",
    "        # Confidence interval for difference\n",
    "        diff_mean = treatment_metric - control_metric\n",
    "        diff_se = np.sqrt(control_returns.var() / len(control_returns) + \n",
    "                         treatment_returns.var() / len(treatment_returns))\n",
    "        \n",
    "        t_critical = stats.t.ppf(1 - self.alpha/2, len(control_returns) + len(treatment_returns) - 2)\n",
    "        ci_lower = diff_mean - t_critical * diff_se * np.sqrt(252)  # Annualized\n",
    "        ci_upper = diff_mean + t_critical * diff_se * np.sqrt(252)\n",
    "        \n",
    "        return {\n",
    "            'control_value': control_metric,\n",
    "            'treatment_value': treatment_metric,\n",
    "            'absolute_difference': treatment_metric - control_metric,\n",
    "            'relative_difference': (treatment_metric - control_metric) / abs(control_metric) if control_metric != 0 else 0,\n",
    "            'cohens_d': cohens_d,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': t_p_value,\n",
    "            'u_statistic': u_stat,\n",
    "            'u_p_value': u_p_value,\n",
    "            'confidence_interval': [ci_lower, ci_upper],\n",
    "            'statistically_significant': t_p_value < self.alpha,\n",
    "            'practically_significant': abs(cohens_d) > 0.2  # Small effect size threshold\n",
    "        }\n",
    "    \n",
    "    def _calculate_max_drawdown(self, returns):\n",
    "        \"\"\"Calculate maximum drawdown for a return series\"\"\"\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdown = (cumulative - running_max) / running_max\n",
    "        return drawdown.min()\n",
    "    \n",
    "    def _generate_conclusion(self, results):\n",
    "        \"\"\"Generate business-focused conclusion from test results\"\"\"\n",
    "        significant_metrics = [m for m in results['metrics'] \n",
    "                             if results['metrics'][m]['significant_after_correction']]\n",
    "        \n",
    "        if len(significant_metrics) == 0:\n",
    "            return {\n",
    "                'recommendation': 'No significant difference detected',\n",
    "                'confidence': 'High',\n",
    "                'business_action': 'Continue with current strategy',\n",
    "                'risk_assessment': 'Low risk to maintain status quo'\n",
    "            }\n",
    "        \n",
    "        # Analyze improvement direction\n",
    "        improvements = []\n",
    "        for metric in significant_metrics:\n",
    "            diff = results['metrics'][metric]['absolute_difference']\n",
    "            if (metric in ['returns', 'sharpe'] and diff > 0) or \\\n",
    "               (metric in ['volatility', 'drawdown'] and diff < 0):\n",
    "                improvements.append(metric)\n",
    "        \n",
    "        if len(improvements) >= len(significant_metrics) * 0.7:  # 70% of metrics improved\n",
    "            recommendation = 'Adopt new strategy'\n",
    "            business_action = 'Implement treatment strategy'\n",
    "        else:\n",
    "            recommendation = 'Mixed results - proceed with caution'\n",
    "            business_action = 'Further testing recommended'\n",
    "        \n",
    "        return {\n",
    "            'recommendation': recommendation,\n",
    "            'significant_metrics': significant_metrics,\n",
    "            'improved_metrics': improvements,\n",
    "            'confidence': 'High' if len(improvements) > 0 else 'Medium',\n",
    "            'business_action': business_action\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ PortfolioABTester class defined\")\n",
    "print(\"üß™ Ready for statistical experimentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a829a9",
   "metadata": {},
   "source": [
    "## üìä **3. Real Data Collection for Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eec1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test portfolios for A/B testing\n",
    "TEST_PORTFOLIOS = {\n",
    "    'control_equal_weight': {\n",
    "        'name': 'Equal Weight Portfolio (Control)',\n",
    "        'description': 'Baseline equal-weight diversified portfolio',\n",
    "        'tickers': ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'JPM', 'JNJ', 'WMT'],\n",
    "        'strategy': 'equal_weight'\n",
    "    },\n",
    "    'treatment_ml_optimized': {\n",
    "        'name': 'ML-Optimized Portfolio (Treatment)',\n",
    "        'description': 'Machine learning enhanced portfolio optimization',\n",
    "        'tickers': ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'JPM', 'JNJ', 'WMT'],\n",
    "        'strategy': 'max_sharpe'\n",
    "    },\n",
    "    'treatment_risk_parity': {\n",
    "        'name': 'Risk Parity Portfolio (Treatment)',\n",
    "        'description': 'Equal risk contribution strategy',\n",
    "        'tickers': ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'JPM', 'JNJ', 'WMT'],\n",
    "        'strategy': 'risk_parity'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Data collection period\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2024-08-01'\n",
    "REBALANCE_FREQUENCY = 'Q'  # Quarterly rebalancing\n",
    "\n",
    "print(f\"üéØ A/B Test Setup:\")\n",
    "print(f\"   üìÖ Test Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   üîÑ Rebalancing: {REBALANCE_FREQUENCY}\")\n",
    "print(f\"   üìä Strategies: {len(TEST_PORTFOLIOS)}\")\n",
    "\n",
    "for key, portfolio in TEST_PORTFOLIOS.items():\n",
    "    print(f\"     - {portfolio['name']}: {portfolio['strategy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804fb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_portfolio_strategy(tickers, strategy, start_date, end_date, rebalance_freq='Q'):\n",
    "    \"\"\"\n",
    "    Simulate portfolio performance with periodic rebalancing\n",
    "    Returns daily portfolio returns for A/B testing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download price data\n",
    "        print(f\"‚è≥ Downloading data for {strategy} strategy...\")\n",
    "        data = yf.download(tickers, start=start_date, end=end_date, auto_adjust=True, progress=False)\n",
    "        \n",
    "        if len(tickers) == 1:\n",
    "            prices = data['Close'].to_frame()\n",
    "            prices.columns = tickers\n",
    "        else:\n",
    "            prices = data['Close']\n",
    "        \n",
    "        # Clean data\n",
    "        prices = prices.dropna()\n",
    "        returns = prices.pct_change().dropna()\n",
    "        \n",
    "        # Generate rebalancing dates\n",
    "        rebalance_dates = pd.date_range(start=start_date, end=end_date, freq=rebalance_freq)\n",
    "        rebalance_dates = [d for d in rebalance_dates if d in prices.index]\n",
    "        \n",
    "        portfolio_returns = []\n",
    "        \n",
    "        for i, rebal_date in enumerate(rebalance_dates[:-1]):\n",
    "            # Get data window for optimization\n",
    "            end_date_window = rebal_date\n",
    "            start_date_window = rebal_date - pd.DateOffset(years=2)  # 2-year lookback\n",
    "            \n",
    "            window_prices = prices[start_date_window:end_date_window]\n",
    "            window_returns = window_prices.pct_change().dropna()\n",
    "            \n",
    "            if len(window_returns) < 250:  # Minimum 1 year of data\n",
    "                continue\n",
    "            \n",
    "            # Calculate portfolio weights based on strategy\n",
    "            if strategy == 'equal_weight':\n",
    "                weights = np.ones(len(tickers)) / len(tickers)\n",
    "                \n",
    "            elif strategy == 'max_sharpe' or strategy == 'risk_parity':\n",
    "                try:\n",
    "                    optimizer = PortfolioOptimizer(tickers, lookback_years=2, use_random_state=True)\n",
    "                    optimizer.prices = window_prices\n",
    "                    optimizer.returns = window_returns\n",
    "                    \n",
    "                    # Calculate expected returns and covariance\n",
    "                    expected_returns = window_returns.mean() * 252\n",
    "                    cov_matrix = window_returns.cov() * 252\n",
    "                    \n",
    "                    weights = optimizer.optimize_portfolio(expected_returns, cov_matrix, method=strategy)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Optimization failed for {rebal_date}, using equal weights: {e}\")\n",
    "                    weights = np.ones(len(tickers)) / len(tickers)\n",
    "            \n",
    "            # Calculate returns for holding period\n",
    "            next_rebal_date = rebalance_dates[i + 1]\n",
    "            holding_period_returns = returns[rebal_date:next_rebal_date]\n",
    "            \n",
    "            # Portfolio returns = weighted sum of asset returns\n",
    "            period_portfolio_returns = (holding_period_returns * weights).sum(axis=1)\n",
    "            portfolio_returns.extend(period_portfolio_returns.tolist())\n",
    "        \n",
    "        # Convert to pandas Series\n",
    "        portfolio_returns_series = pd.Series(portfolio_returns)\n",
    "        \n",
    "        print(f\"‚úÖ {strategy} simulation complete: {len(portfolio_returns_series)} days\")\n",
    "        return portfolio_returns_series\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {strategy} simulation: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "# Simulate all portfolio strategies\n",
    "portfolio_returns = {}\n",
    "\n",
    "for key, portfolio_config in TEST_PORTFOLIOS.items():\n",
    "    returns = simulate_portfolio_strategy(\n",
    "        portfolio_config['tickers'],\n",
    "        portfolio_config['strategy'],\n",
    "        START_DATE,\n",
    "        END_DATE,\n",
    "        REBALANCE_FREQUENCY\n",
    "    )\n",
    "    \n",
    "    if len(returns) > 0:\n",
    "        portfolio_returns[key] = returns\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        annual_return = returns.mean() * 252\n",
    "        annual_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        \n",
    "        print(f\"üìä {portfolio_config['name']}:\")\n",
    "        print(f\"     Annual Return: {annual_return:.2%}\")\n",
    "        print(f\"     Volatility: {annual_vol:.2%}\")\n",
    "        print(f\"     Sharpe Ratio: {sharpe:.2f}\")\n",
    "        print(f\"     Trading Days: {len(returns)}\")\n",
    "        print()\n",
    "\n",
    "print(f\"‚úÖ Portfolio simulations complete\")\n",
    "print(f\"üìä Ready for A/B testing with {len(portfolio_returns)} strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b910d",
   "metadata": {},
   "source": [
    "## üß™ **4. Execute A/B Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32826331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize A/B testing framework\n",
    "ab_tester = PortfolioABTester(confidence_level=0.95, minimum_effect_size=0.02)\n",
    "\n",
    "# Define experiments to run\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        'name': 'ML Optimization vs Equal Weight',\n",
    "        'control': 'control_equal_weight',\n",
    "        'treatment': 'treatment_ml_optimized',\n",
    "        'hypothesis': 'ML optimization improves risk-adjusted returns',\n",
    "        'business_question': 'Should we replace equal-weight with ML optimization?'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Risk Parity vs Equal Weight',\n",
    "        'control': 'control_equal_weight',\n",
    "        'treatment': 'treatment_risk_parity',\n",
    "        'hypothesis': 'Risk parity provides better risk management',\n",
    "        'business_question': 'Does risk parity reduce drawdowns significantly?'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ML Optimization vs Risk Parity',\n",
    "        'control': 'treatment_risk_parity',\n",
    "        'treatment': 'treatment_ml_optimized',\n",
    "        'hypothesis': 'ML optimization outperforms risk parity',\n",
    "        'business_question': 'Which advanced strategy should we deploy?'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Calculate sample sizes for power analysis\n",
    "print(\"üìä Statistical Power Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    if exp['control'] in portfolio_returns and exp['treatment'] in portfolio_returns:\n",
    "        # Estimate effect size from data\n",
    "        control_returns = portfolio_returns[exp['control']]\n",
    "        treatment_returns = portfolio_returns[exp['treatment']]\n",
    "        \n",
    "        # Calculate observed effect size\n",
    "        pooled_std = np.sqrt((control_returns.var() + treatment_returns.var()) / 2)\n",
    "        observed_effect = abs(treatment_returns.mean() - control_returns.mean()) / pooled_std\n",
    "        \n",
    "        sample_size_info = ab_tester.calculate_sample_size(effect_size=observed_effect, power=0.8)\n",
    "        \n",
    "        print(f\"\\nüß™ {exp['name']}:\")\n",
    "        print(f\"   Observed Effect Size: {observed_effect:.3f}\")\n",
    "        print(f\"   Required Sample Size: {sample_size_info['total_sample_size']} observations\")\n",
    "        print(f\"   Actual Sample Size: {min(len(control_returns), len(treatment_returns))} observations\")\n",
    "        \n",
    "        adequately_powered = min(len(control_returns), len(treatment_returns)) >= sample_size_info['total_sample_size']\n",
    "        print(f\"   Statistical Power: {'‚úÖ Adequate' if adequately_powered else '‚ö†Ô∏è Underpowered'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all A/B tests\n",
    "test_results = {}\n",
    "\n",
    "print(\"üß™ Executing A/B Tests:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    if exp['control'] in portfolio_returns and exp['treatment'] in portfolio_returns:\n",
    "        print(f\"\\nüìä Running: {exp['name']}\")\n",
    "        print(f\"   Hypothesis: {exp['hypothesis']}\")\n",
    "        print(f\"   Business Question: {exp['business_question']}\")\n",
    "        \n",
    "        # Get returns data\n",
    "        control_returns = portfolio_returns[exp['control']]\n",
    "        treatment_returns = portfolio_returns[exp['treatment']]\n",
    "        \n",
    "        # Ensure same length (align on dates)\n",
    "        min_length = min(len(control_returns), len(treatment_returns))\n",
    "        control_returns = control_returns[:min_length]\n",
    "        treatment_returns = treatment_returns[:min_length]\n",
    "        \n",
    "        # Run A/B test\n",
    "        result = ab_tester.run_experiment(\n",
    "            control_returns,\n",
    "            treatment_returns,\n",
    "            experiment_name=exp['name'],\n",
    "            metrics=['returns', 'sharpe', 'volatility', 'drawdown']\n",
    "        )\n",
    "        \n",
    "        test_results[exp['name']] = result\n",
    "        \n",
    "        # Print key results\n",
    "        print(f\"\\n   üìà Results Summary:\")\n",
    "        for metric, metric_result in result['metrics'].items():\n",
    "            control_val = metric_result['control_value']\n",
    "            treatment_val = metric_result['treatment_value']\n",
    "            p_val = metric_result['p_value_corrected']\n",
    "            significant = metric_result['significant_after_correction']\n",
    "            \n",
    "            print(f\"     {metric.capitalize()}:\")\n",
    "            print(f\"       Control: {control_val:.4f} | Treatment: {treatment_val:.4f}\")\n",
    "            print(f\"       Difference: {treatment_val - control_val:+.4f} (p={p_val:.4f})\")\n",
    "            print(f\"       Significant: {'‚úÖ Yes' if significant else '‚ùå No'}\")\n",
    "        \n",
    "        # Business conclusion\n",
    "        conclusion = result['overall_conclusion']\n",
    "        print(f\"\\n   üéØ Business Recommendation: {conclusion['recommendation']}\")\n",
    "        print(f\"   üìã Action: {conclusion['business_action']}\")\n",
    "        print(f\"   üìä Confidence: {conclusion['confidence']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ All A/B tests completed\")\n",
    "print(f\"üìä Total experiments: {len(test_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4fee2",
   "metadata": {},
   "source": [
    "## üìä **5. Professional Results Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79764b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive A/B testing dashboard\n",
    "def create_ab_testing_dashboard(test_results, portfolio_returns):\n",
    "    \"\"\"\n",
    "    Create executive-ready A/B testing visualization dashboard\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Portfolio Performance Comparison',\n",
    "            'Statistical Significance Summary',\n",
    "            'Effect Sizes (Cohen\\'s d)',\n",
    "            'Confidence Intervals'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Performance comparison (cumulative returns)\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    for i, (strategy, returns) in enumerate(portfolio_returns.items()):\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=list(range(len(cumulative))),\n",
    "                y=cumulative,\n",
    "                mode='lines',\n",
    "                name=TEST_PORTFOLIOS[strategy]['name'],\n",
    "                line=dict(color=colors[i % len(colors)], width=2)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Statistical significance summary\n",
    "    experiments = list(test_results.keys())\n",
    "    significant_counts = []\n",
    "    \n",
    "    for exp_name in experiments:\n",
    "        result = test_results[exp_name]\n",
    "        significant_count = sum(1 for m in result['metrics'].values() \n",
    "                              if m['significant_after_correction'])\n",
    "        significant_counts.append(significant_count)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=experiments,\n",
    "            y=significant_counts,\n",
    "            name='Significant Metrics',\n",
    "            marker_color='green',\n",
    "            text=significant_counts,\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Effect sizes\n",
    "    effect_sizes = []\n",
    "    metric_names = []\n",
    "    exp_names = []\n",
    "    \n",
    "    for exp_name, result in test_results.items():\n",
    "        for metric_name, metric_result in result['metrics'].items():\n",
    "            effect_sizes.append(abs(metric_result['cohens_d']))\n",
    "            metric_names.append(f\"{exp_name}\\n{metric_name}\")\n",
    "            exp_names.append(exp_name)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=metric_names,\n",
    "            y=effect_sizes,\n",
    "            name='Effect Size',\n",
    "            marker_color='orange',\n",
    "            text=[f'{es:.2f}' for es in effect_sizes],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Confidence intervals for Sharpe ratios\n",
    "    exp_names_ci = []\n",
    "    sharpe_differences = []\n",
    "    ci_lowers = []\n",
    "    ci_uppers = []\n",
    "    \n",
    "    for exp_name, result in test_results.items():\n",
    "        if 'sharpe' in result['metrics']:\n",
    "            sharpe_metric = result['metrics']['sharpe']\n",
    "            exp_names_ci.append(exp_name)\n",
    "            sharpe_differences.append(sharpe_metric['absolute_difference'])\n",
    "            ci_lowers.append(sharpe_metric['confidence_interval'][0])\n",
    "            ci_uppers.append(sharpe_metric['confidence_interval'][1])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=exp_names_ci,\n",
    "            y=sharpe_differences,\n",
    "            mode='markers',\n",
    "            name='Sharpe Difference',\n",
    "            marker=dict(size=10, color='blue'),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                symmetric=False,\n",
    "                array=[u - d for u, d in zip(ci_uppers, sharpe_differences)],\n",
    "                arrayminus=[d - l for l, d in zip(ci_lowers, sharpe_differences)]\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Add horizontal line at zero for reference\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Portfolio A/B Testing Results Dashboard - FAANG Data Analytics\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text=\"Trading Days\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Return\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Experiment\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Significant Metrics Count\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Metric\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Effect Size (|Cohen's d|)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Experiment\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Sharpe Ratio Difference\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generate dashboard\n",
    "if len(test_results) > 0 and len(portfolio_returns) > 0:\n",
    "    dashboard = create_ab_testing_dashboard(test_results, portfolio_returns)\n",
    "    dashboard.show()\n",
    "    \n",
    "    print(\"üìä A/B Testing Dashboard Generated\")\n",
    "    print(\"   ‚úÖ Comprehensive statistical analysis visualization\")\n",
    "    print(\"   ‚úÖ Executive-ready presentation format\")\n",
    "    print(\"   ‚úÖ Publication-quality charts for stakeholders\")\n",
    "else:\n",
    "    print(\"‚ùå Insufficient data for dashboard generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df30ec",
   "metadata": {},
   "source": [
    "## üéØ **6. Executive Summary & Business Intelligence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive executive summary\n",
    "def generate_ab_testing_executive_summary(test_results, portfolio_returns):\n",
    "    \"\"\"\n",
    "    Generate business-focused executive summary of A/B testing results\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'analysis_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'testing_period': f\"{START_DATE} to {END_DATE}\",\n",
    "        'total_experiments': len(test_results),\n",
    "        'statistical_confidence': '95%',\n",
    "        'multiple_testing_correction': 'Bonferroni'\n",
    "    }\n",
    "    \n",
    "    # Strategy performance summary\n",
    "    strategy_performance = {}\n",
    "    for strategy, returns in portfolio_returns.items():\n",
    "        annual_return = returns.mean() * 252\n",
    "        annual_vol = returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "        max_dd = ab_tester._calculate_max_drawdown(returns)\n",
    "        \n",
    "        strategy_performance[strategy] = {\n",
    "            'annual_return': annual_return,\n",
    "            'volatility': annual_vol,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'total_days': len(returns)\n",
    "        }\n",
    "    \n",
    "    # Experiment conclusions\n",
    "    experiment_conclusions = {}\n",
    "    for exp_name, result in test_results.items():\n",
    "        significant_metrics = [m for m in result['metrics'] \n",
    "                             if result['metrics'][m]['significant_after_correction']]\n",
    "        \n",
    "        experiment_conclusions[exp_name] = {\n",
    "            'recommendation': result['overall_conclusion']['recommendation'],\n",
    "            'significant_metrics': significant_metrics,\n",
    "            'confidence': result['overall_conclusion']['confidence'],\n",
    "            'business_action': result['overall_conclusion']['business_action']\n",
    "        }\n",
    "    \n",
    "    summary.update({\n",
    "        'strategy_performance': strategy_performance,\n",
    "        'experiment_conclusions': experiment_conclusions\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "if len(test_results) > 0:\n",
    "    executive_summary = generate_ab_testing_executive_summary(test_results, portfolio_returns)\n",
    "    \n",
    "    print(\"üéØ EXECUTIVE SUMMARY - A/B TESTING RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìÖ Analysis Date: {executive_summary['analysis_date']}\")\n",
    "    print(f\"üìä Testing Period: {executive_summary['testing_period']}\")\n",
    "    print(f\"üß™ Experiments Conducted: {executive_summary['total_experiments']}\")\n",
    "    print(f\"üìà Statistical Confidence: {executive_summary['statistical_confidence']}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ STRATEGY PERFORMANCE RANKING:\")\n",
    "    # Sort by Sharpe ratio\n",
    "    sorted_strategies = sorted(executive_summary['strategy_performance'].items(), \n",
    "                              key=lambda x: x[1]['sharpe_ratio'], reverse=True)\n",
    "    \n",
    "    for i, (strategy, perf) in enumerate(sorted_strategies, 1):\n",
    "        strategy_name = TEST_PORTFOLIOS.get(strategy, {}).get('name', strategy)\n",
    "        print(f\"   {i}. {strategy_name}:\")\n",
    "        print(f\"      Return: {perf['annual_return']:.2%} | Volatility: {perf['volatility']:.2%}\")\n",
    "        print(f\"      Sharpe: {perf['sharpe_ratio']:.2f} | Max Drawdown: {perf['max_drawdown']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nüî¨ EXPERIMENT CONCLUSIONS:\")\n",
    "    for exp_name, conclusion in executive_summary['experiment_conclusions'].items():\n",
    "        print(f\"\\n   üìä {exp_name}:\")\n",
    "        print(f\"      Recommendation: {conclusion['recommendation']}\")\n",
    "        print(f\"      Action: {conclusion['business_action']}\")\n",
    "        print(f\"      Confidence: {conclusion['confidence']}\")\n",
    "        if conclusion['significant_metrics']:\n",
    "            print(f\"      Significant Metrics: {', '.join(conclusion['significant_metrics'])}\")\n",
    "        else:\n",
    "            print(f\"      Significant Metrics: None (no statistically significant differences)\")\n",
    "    \n",
    "    print(f\"\\nüíº BUSINESS IMPLICATIONS:\")\n",
    "    print(f\"   ‚úÖ Rigorous statistical validation of investment strategies\")\n",
    "    print(f\"   ‚úÖ Risk-controlled experimentation framework\")\n",
    "    print(f\"   ‚úÖ Data-driven portfolio optimization decisions\")\n",
    "    print(f\"   ‚úÖ Institutional-grade testing methodology\")\n",
    "    \n",
    "    print(f\"\\nüéØ FAANG INTERVIEW TALKING POINTS:\")\n",
    "    print(f\"   üìä 'Designed and executed {executive_summary['total_experiments']} A/B tests for portfolio strategies'\")\n",
    "    print(f\"   üî¨ 'Applied rigorous statistical methodology with {executive_summary['statistical_confidence']} confidence'\")\n",
    "    print(f\"   üìà 'Validated {len([c for c in executive_summary['experiment_conclusions'].values() if c['confidence'] == 'High'])} high-confidence strategy improvements'\")\n",
    "    print(f\"   üíº 'Generated actionable insights for portfolio optimization and risk management'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ A/B TESTING ANALYSIS COMPLETE\")\n",
    "    print(\"üìù Ready for stakeholder presentation and production deployment\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No A/B testing results available for summary generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240a327",
   "metadata": {},
   "source": [
    "## üìö **7. Next Steps & Production Integration**\n",
    "\n",
    "### üîÑ **Recommended Follow-up Actions**\n",
    "1. **Implement Winner**: Deploy statistically significant strategies to production\n",
    "2. **Continuous Testing**: Set up automated A/B testing pipeline\n",
    "3. **Real Money Validation**: Small-scale real capital deployment\n",
    "4. **Advanced Experiments**: Multi-arm bandits, sequential testing\n",
    "\n",
    "### üéØ **FAANG Interview Preparation**\n",
    "- **Experimental Design**: Proper randomization, control groups, sample size calculations\n",
    "- **Statistical Rigor**: Multiple testing corrections, confidence intervals, effect sizes\n",
    "- **Business Impact**: ROI quantification, risk assessment, stakeholder communication\n",
    "- **Production Readiness**: Scalable framework, automated reporting, monitoring\n",
    "\n",
    "### üìä **Integration with Other Notebooks**\n",
    "- **Statistical Foundation** (`01_statistical_foundation.ipynb`): Provides underlying statistical framework\n",
    "- **Performance Attribution** (`05_performance_attribution.ipynb`): Decompose A/B test results\n",
    "- **Risk Analytics** (`06_risk_analytics_deep_dive.ipynb`): Analyze risk implications of strategy changes\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ This notebook demonstrates production-grade A/B testing capabilities essential for FAANG data analyst positions.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
